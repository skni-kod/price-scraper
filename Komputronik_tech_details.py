import csv
import json
import glob
import os
import time
from datetime import datetime
from loguru import logger
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.service import Service
from config import (driver, date, setup_logging)

# set output_folder to folder created in config.py
output_folder = "output"
shop_name = "komputronik"

csv_filename = os.path.join(output_folder, f"{shop_name}_{date}.csv")
log_filename = os.path.join(output_folder, f"log_tech_details_{shop_name}_{date}.log")

# Setup logger
logger = setup_logging(log_filename)
logger.info("Starting technical details retrieval script.")
logger.info("CSV File: {}", csv_filename)
logger.info("Log File: {}", log_filename)

# Find the latest CSV file generated by the first script
csv_pattern = os.path.join(output_folder, f"{shop_name}_*.csv")
csv_files = glob.glob(csv_pattern)
if not csv_files:
    logger.error("No CSV files found matching the pattern {}", csv_pattern)
    exit(1)

def extract_date(filename):
    """
    Extracts date from the filename assuming the format shop_name_dd-mm-yyyy-hh-mm-ss.csv.
    """
    base = os.path.basename(filename)
    # File name format: {shop_name}_dd-mm-yyyy-hh-mm-ss.csv
    date_str = base.replace(f"{shop_name}_", "").replace(".csv", "")
    try:
        return datetime.strptime(date_str, "%d-%m-%Y-%H-%M-%S")
    except ValueError:
        return None

csv_files_with_date = [(file, extract_date(file)) for file in csv_files if extract_date(file) is not None]
if not csv_files_with_date:
    logger.error("None of the CSV files have a valid date format.")
    exit(1)
csv_files_with_date.sort(key=lambda x: x[1], reverse=True)
latest_csv_file = csv_files_with_date[0][0]
logger.info("Selected CSV file: {}", latest_csv_file)

# Load product links from the CSV file
product_data = []
with open(latest_csv_file, mode="r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        if row.get("product_link"):
            product_data.append({
                "product_link": row["product_link"],
            })
logger.info("Found {} products to process.", len(product_data))

# Configure Selenium (Firefox, Geckodriver)
service = Service("/usr/local/bin/geckodriver")
options = webdriver.FirefoxOptions()
options.add_argument("--headless")
driver = webdriver.Firefox(service=service, options=options)

def scrape_tech_details(url):
    """
    This function opens a product page and attempts to retrieve all technical details.
    """
    tech_details = {}
    try:
        driver.get(url)
        time.sleep(2)  # short wait for the page to load

        attributes_container = driver.find_element(By.XPATH, '//div[@data-name="productAttributes"]')
        detail_elements = attributes_container.find_elements(
            By.XPATH, './/div[contains(@class, "mt-4") or contains(@class, "space-y-2")]'
        )
        for element in detail_elements:
            try:
                p_elements = element.find_elements(By.TAG_NAME, 'p')
                label_elements = element.find_elements(By.TAG_NAME, 'label')
                if p_elements and label_elements:
                    key = p_elements[0].text.replace(":", "").strip()
                    checked_label = None
                    for label in label_elements:
                        try:
                            input_el = label.find_element(By.TAG_NAME, 'input')
                            if input_el.get_attribute("checked") is not None:
                                checked_label = label
                                break
                        except Exception:
                            continue
                    if checked_label:
                        value = checked_label.find_element(By.TAG_NAME, 'span').text.strip()
                    else:
                        value = label_elements[0].find_element(By.TAG_NAME, 'span').text.strip()
                    tech_details[key] = value
                else:
                    span_elements = element.find_elements(By.TAG_NAME, 'span')
                    if len(span_elements) >= 2:
                        key = span_elements[0].text.replace(":", "").strip()
                        value = span_elements[1].text.strip()
                        tech_details[key] = value
            except Exception as inner_e:
                logger.info("Error processing detail: {}", inner_e)
    except Exception as e:
        logger.error("Error opening URL {}: {}", url, e)
    return tech_details

# Prepare the output CSV file for technical details in the output folder
tech_csv_filename = os.path.join(output_folder, f"tech_details_{shop_name}_{date}.csv")
fieldnames = ["product_link", "tech_details"]

with open(tech_csv_filename, mode="w", newline="", encoding="utf-8") as tech_csvfile:
    writer = csv.DictWriter(tech_csvfile, fieldnames=fieldnames)
    writer.writeheader()
    for item in product_data:
        url = item["product_link"]
        logger.info("Processing: {}", url)
        details = scrape_tech_details(url)
        writer.writerow({
            "product_link": url,
            "tech_details": json.dumps(details, ensure_ascii=False)
        })

# Close the browser
driver.quit()
logger.complete()
logger.info("Technical details retrieval completed. Data saved in file: {}", tech_csv_filename)