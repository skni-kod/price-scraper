import glob
import os
import csv
import json
from datetime import datetime
from loguru import logger

from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from config import (driver, date, setup_logging)

# set output_folder to folder created in config.py
output_folder = "output"
shop_name = "rtv_euro_agd"

csv_filename = os.path.join(output_folder, f"{shop_name}_{date}.csv")
log_filename = os.path.join(output_folder, f"log_tech_details_{shop_name}_{date}.log")

# Setup logger
logger = setup_logging(log_filename)
logger.info("Starting technical details retrieval script.")
logger.info("CSV File: {}", csv_filename)
logger.info("Log File: {}", log_filename)

# Find the latest CSV file generated by the first script
csv_pattern = os.path.join(output_folder, f"{shop_name}_*.csv")
csv_files = glob.glob(csv_pattern)
if not csv_files:
    logger.error("No CSV files found matching the pattern {}", csv_pattern)
    exit(1)

def extract_date(filename):
    """
    Extracts date from the filename assuming the format shop_name_dd-mm-yyyy-hh-mm-ss.csv.
    """
    base = os.path.basename(filename)
    # File name format: {shop_name}_dd-mm-yyyy-hh-mm-ss.csv
    date_str = base.replace(f"{shop_name}_", "").replace(".csv", "")
    try:
        return datetime.strptime(date_str, "%d-%m-%Y-%H-%M-%S")
    except ValueError:
        return None 
    
csv_files_with_date = [(file, extract_date(file)) for file in csv_files if extract_date(file) is not None]
if not csv_files_with_date:
    logger.error("None of the CSV files have a valid date format.")
    exit(1)
csv_files_with_date.sort(key=lambda x: x[1], reverse=True)
latest_csv_file = csv_files_with_date[0][0]
logger.info("Selected CSV file: {}", latest_csv_file)

# Load product links from the CSV file
product_data = []
with open(latest_csv_file, mode="r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        if row.get("product_link"):
            product_data.append({
                "product_link": row["product_link"],
            })
logger.info("Found {} products to process.", len(product_data))

def scrape_tech_details(url):
    """
    This function opens a product page and attempts to retrieve all technical details.

    """
    driver.get(url)
    
    #Waiting for "technical-attributes" div to show up
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[@class="technical-attributes"]'))
        )
    except TimeoutException:
        logger.error("Page failed to load within 10 seconds.")
        return {}

    tech_details = {}
    try:
        # Handle cookie consent overlay if present
        overlay = driver.find_elements(By.CLASS_NAME, "onetrust-pc-dark-filter")
        if overlay:
            WebDriverWait(driver, 3).until(
                EC.staleness_of(overlay[0])
            )
            
        # Accept cookies if button exists
        cookie_buttons = driver.find_elements(By.XPATH, '//button[contains(@id, "onetrust-accept-btn-handler")]')
        if cookie_buttons:
            driver.execute_script("arguments[0].click();", cookie_buttons[0])

        # Check if "Rozwiń pełne dane techniczne" is present and wait for overlay to fade if present
        try:
            if overlay:
                WebDriverWait(driver, 3).until(
                EC.staleness_of(overlay[0])
            )
            show_more_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, '//button[contains(@class, "cta") and .//span[contains(text(), "Rozwiń pełne dane techniczne")]]'))
            )
            driver.execute_script("arguments[0].scrollIntoView();", show_more_button)
            show_more_button.click()
        except TimeoutException:
            logger.warning("Button 'Rozwiń pełne dane techniczne' not found")

        attributes_container = driver.find_element(By.XPATH, '//div[@class="technical-attributes"]')
        detail_elements = attributes_container.find_elements(By.XPATH, './/div[@class="technical-attributes__section"]')
        for element in detail_elements:
            try:
                tr_elements = element.find_elements(By.TAG_NAME, 'tr')
                if tr_elements:
                    for tr_element in tr_elements:
                        #Skip links to warranties etc.
                        if tr_element.find_elements(By.TAG_NAME, 'a'):
                            logger.info(f"Element skipped  '{key}' because it contains a link.")
                            continue
                        try:
                            key_element = tr_element.find_elements(By.TAG_NAME, 'th')
                            value_element = tr_element.find_elements(By.TAG_NAME, 'span')

                            if key_element and value_element:
                                key = key_element[0].text.replace(":", "").strip()
                                value = value_element[0].text.strip()
                                tech_details[key] = value
                            else:
                                logger.warning("Line skipped because 'th' or 'span' is missing.")
                        except Exception as inner_e:
                            logger.warning("Error processing detail: {}", inner_e)
            except Exception as inner_e:
                logger.warning("Error processing detail: {}", inner_e)
    except Exception as e:
        logger.error("Error opening URL {}: {}", url, e)
    return tech_details        

# Prepare the output CSV file for technical details in the output folder
tech_csv_filename = os.path.join(output_folder, f"tech_details_{shop_name}_{date}.csv")
fieldnames = ["product_link", "tech_details"]

with open(tech_csv_filename, mode="w", newline="", encoding="utf-8") as tech_csvfile:
    writer = csv.DictWriter(tech_csvfile, fieldnames=fieldnames)
    writer.writeheader()
    for item in product_data:
        url = item["product_link"]
        logger.info("Processing: {}", url)
        details = scrape_tech_details(url)
        writer.writerow({
            "product_link": url,
            "tech_details": json.dumps(details, ensure_ascii=False)
        })

driver.quit()
logger.complete()
logger.info("Technical details retrieval completed. Data saved in file: {}", tech_csv_filename)